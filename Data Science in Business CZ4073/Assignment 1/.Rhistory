knitr::opts_chunk$set(echo = TRUE)
cor(navalDataRefined1)
cor(navalDataRefined1)
navalDataRefined1 <- navalData[-c(9,12)]
navalData <- read.csv("assign1_NavalData.csv", header = TRUE)
navalDataRefined1 <- navalData[-c(9,12)]
cor(navalDataRefined1)
library(GGally)
ggcorr(navalDataRefined1, geom="tile")
# Plotting two-dimensional scatterplots of all pairs of variables
pairs(navalDataRefined1, pch = 19, col = "orange")
set.seed(1)
row.number <- sample(1:nrow(navalDataRefined2),0.8*nrow(navalDataRefined2))
train = navalDataRefined2[row.number,]
test  = navalDataRefined2[-row.number,]
navalDataRefined2 <- navalData[-c(9,12,7)]
set.seed(1)
row.number <- sample(1:nrow(navalDataRefined2),0.8*nrow(navalDataRefined2))
train = navalDataRefined2[row.number,]
test  = navalDataRefined2[-row.number,]
#adjusting data and initializing training data and test data for the linear model to be built
navalDataRefined2 <- navalData[-c(9,12,7)]
set.seed(1)
row.number <- sample(1:nrow(navalDataRefined2),0.8*nrow(navalDataRefined2))
trainData = navalDataRefined2[row.number,]
testData  = navalDataRefined2[-row.number,]
lmFitY1_Attempt1 <- lm(Y1 ~ .,data = trainData[,-15])
summary(lmFitY1_Attempt1)
lmFitY1_Attempt1 <- lm(Y1 ~ .,data = trainData[,-15])
summary(lmFitY1_Attempt1)
step(lm, direction = "backward")
lmFitY1_Attempt1 <- lm(Y1 ~ .,data = trainData[,-15])
summary(lmFitY1_Attempt1)
step(lmFitY1_Attempt1, direction = "backward")
lmFitY1_Attempt2 <- update(lmFitY1_Attempt1, ~ . - X4, data = train[,-15])
summary(lmFitY1_Attempt2)
# For univariate and bivariate model, outlier can be detected and treated using boxplots. For multi-variate model we use Cook's distance
cooksd <- cooks.distance(lmFitY1_Attempt2)
plot(cooksd, pch="*", cex=2, main="Influential Observations")  # plot cook's distance
abline(h = 4*mean(cooksd, na.rm=T), col="red")  # add cutoff line
text(x=1:length(cooksd)+1, y=cooksd, labels=ifelse(cooksd>4*mean(cooksd, na.rm=T),names(cooksd),""), col="red")  # add labels
# For univariate and bivariate model, outlier can be detected and treated using boxplots. For multi-variate model we use Cook's distance
cooksd <- cooks.distance(lmFitY1_Attempt2)
plot(cooksd, pch="*", cex=2, main="Influential Observations")  # plot cook's distance
abline(h = 4*mean(cooksd, na.rm=T), col="red")  # add cutoff line
text(x=1:length(cooksd)+1, y=cooksd, labels=ifelse(cooksd>6*mean(cooksd, na.rm=T),names(cooksd),""), col="red")  # add labels
# For univariate and bivariate model, outlier can be detected and treated using boxplots. For multi-variate model we use Cook's distance
cooksd <- cooks.distance(lmFitY1_Attempt2)
plot(cooksd, pch="*", cex=2, main="Influential Observations")  # plot cook's distance
abline(h = 10*mean(cooksd, na.rm=T), col="red")  # add cutoff line
text(x=1:length(cooksd)+1, y=cooksd, labels=ifelse(cooksd>10*mean(cooksd, na.rm=T),names(cooksd),""), col="red")  # add labels
# For univariate and bivariate model, outlier can be detected and treated using boxplots. For multi-variate model we use Cook's distance
cooksd <- cooks.distance(lmFitY1_Attempt2)
plot(cooksd, pch="*", cex=2, main="Influential Observations")  # plot cook's distance
abline(h = 10*mean(cooksd, na.rm=T), col="red")  # add cutoff line
text(x=1:length(cooksd)+1, y=cooksd, labels=ifelse(cooksd>10*mean(cooksd, na.rm=T),names(cooksd),""), col="red")  # add labels
# Observations with Cook distance greater than 4/(n - p -1) where n is the number of observations and p is the number of input features should be eliminated to take care of the outliers
navalDataRefined3 <- trainData[abs(cooksd)<4/(nrow(trainData)-1),]
lmFitY1_Attempt2 <- lm(formula(lmFitY1_Attempt2), data = train.clean[,-15])
# For univariate and bivariate model, outlier can be detected and treated using boxplots. For multi-variate model we use Cook's distance
cooksd <- cooks.distance(lmFitY1_Attempt2)
plot(cooksd, pch="*", cex=2, main="Influential Observations")  # plot cook's distance
abline(h = 10*mean(cooksd, na.rm=T), col="red")  # add cutoff line
text(x=1:length(cooksd)+1, y=cooksd, labels=ifelse(cooksd>10*mean(cooksd, na.rm=T),names(cooksd),""), col="red")  # add labels
# Observations with Cook distance greater than 4/(n - p -1) where n is the number of observations and p is the number of input features should be eliminated to take care of the outliers
navalDataRefined3 <- trainData[abs(cooksd)<4/(nrow(trainData)-1),]
lmFitY1_Attempt2 <- lm(formula(lmFitY1_Attempt2), data = navalDataRefined3[,-15])
summary(lmFitY1_Attempt2)
# For univariate and bivariate model, outlier can be detected and treated using boxplots. For multi-variate model we use Cook's distance
cooksd <- cooks.distance(lmFitY1_Attempt2)
plot(cooksd, pch="*", cex=2, main="Influential Observations")  # plot cook's distance
abline(h = 1abs(4/(nrow(trainData)-1)), col="red")  # add cutoff line
# For univariate and bivariate model, outlier can be detected and treated using boxplots. For multi-variate model we use Cook's distance
cooksd <- cooks.distance(lmFitY1_Attempt2)
plot(cooksd, pch="*", cex=2, main="Influential Observations")  # plot cook's distance
abline(h = abs(4/(nrow(trainData)-1)), col="red")  # add cutoff line
text(x=1:length(cooksd)+1, y=cooksd, labels=ifelse(cooksd>abs(4/(nrow(trainData)-1)),names(cooksd),""), col="red")  # add labels
# Observations with Cook distance greater than 4/(n - p -1) where n is the number of observations and p is the number of input features should be eliminated to take care of the outliers
navalDataRefined3 <- trainData[abs(cooksd)<4/(nrow(trainData)-1),]
lmFitY1_Attempt2 <- lm(formula(lmFitY1_Attempt2), data = navalDataRefined3[,-15])
summary(lmFitY1_Attempt2)
lmFitY1_Attempt2 <- update(lmFitY1_Attempt1, ~ . - X4, data = train[,-15])
summary(lmFitY1_Attempt2)
# For univariate and bivariate model, outlier can be detected and treated using boxplots. For multi-variate model we use Cook's distance
cooksd <- cooks.distance(lmFitY1_Attempt2)
plot(cooksd, pch="*", cex=2, main="Influential Observations")  # plot cook's distance
abline(h = abs(4/(nrow(trainData)-1)), col="red")  # add cutoff line
text(x=1:length(cooksd)+1, y=cooksd, labels=ifelse(cooksd>abs(4/(nrow(trainData)-1)),names(cooksd),""), col="red")  # add labels
# Observations with Cook distance greater than 4/(n - p -1) where n is the number of observations and p is the number of input features should be eliminated to take care of the outliers
navalDataRefined3 <- trainData[abs(cooksd)<4/(nrow(trainData)-1),]
lmFitY1_Attempt3 <- lm(formula(lmFitY1_Attempt2), data = navalDataRefined3[,-15])
summary(lmFitY1_Attempt3)
lmFitY1_Attempt4 <- update(lmFitY1_Attempt3, ~ . - X3, data = train[,-15])
summary(lmFitY1_Attempt4)
lmFitY2_Attempt1 <- lm(Y2 ~ .,data = trainData[,-15])
#adjusting data and initializing training data and test data for the linear model to be built
navalDataRefined2 <- navalData[-c(9,12,7)]
set.seed(1)
row.number <- sample(1:nrow(navalDataRefined2),0.8*nrow(navalDataRefined2))
trainData = navalDataRefined2[row.number,]
testData  = navalDataRefined2[-row.number,]
lmFitY2_Attempt1 <- lm(Y2 ~ .,data = trainData[,-15])
#adjusting data and initializing training data and test data for the linear model to be built
navalDataRefined2 <- navalData[-c(9,12,7)]
set.seed(1)
row.number <- sample(1:nrow(navalDataRefined2),0.8*nrow(navalDataRefined2))
trainData = navalDataRefined2[row.number,]
testData  = navalDataRefined2[-row.number,]
lmFitY1_Attempt1 <- lm(Y1 ~ .,data = trainData[,-15])
summary(lmFitY1_Attempt1)
#step(lmFitY1_Attempt1, direction = "backward")
lmFitY1_Attempt2 <- update(lmFitY1_Attempt1, ~ . - X4, data = train[,-15])
summary(lmFitY1_Attempt2)
# For univariate and bivariate model, outlier can be detected and treated using boxplots. For multi-variate model we use Cook's distance
cooksd <- cooks.distance(lmFitY1_Attempt2)
plot(cooksd, pch="*", cex=2, main="Influential Observations")  # plot cook's distance
abline(h = abs(4/(nrow(trainData)-1)), col="red")  # add cutoff line
text(x=1:length(cooksd)+1, y=cooksd, labels=ifelse(cooksd>abs(4/(nrow(trainData)-1)),names(cooksd),""), col="red")  # add labels
# Observations with Cook distance greater than 4/(n - p -1) where n is the number of observations and p is the number of input features should be eliminated to take care of the outliers
navalDataRefined3 <- trainData[abs(cooksd)<4/(nrow(trainData)-1),]
lmFitY1_Attempt3 <- lm(formula(lmFitY1_Attempt2), data = navalDataRefined3[,-15])
summary(lmFitY1_Attempt3)
lmFitY2_Attempt1 <- lm(Y2 ~ .,data = trainData[,-15])
lmFitY1_Attempt4 <- update(lmFitY1_Attempt3, ~ . - X3, data = train[,-14])
lmFitY1_Attempt4 <- update(lmFitY1_Attempt3, ~ . - X3, data = train[,-15])
summary(lmFitY1_Attempt4)
lmFitY2_Attempt1 <- lm(Y2 ~ .,data = trainData[,-14])
summary(lmFitY2_Attempt1)
lmFitY2_Attempt2 <- update(lmFitY2_Attempt1, ~ . - X2, data = train[,-14])
summary(lmFitY2_Attempt2)
# For univariate and bivariate model, outlier can be detected and treated using boxplots. For multi-variate model we use Cook's distance
cooksd <- cooks.distance(lmFitY2_Attempt2)
plot(cooksd, pch="*", cex=2, main="Influential Observations")  # plot cook's distance
abline(h = abs(4/(nrow(trainData)-1)), col="red")  # add cutoff line
text(x=1:length(cooksd)+1, y=cooksd, labels=ifelse(cooksd>abs(4/(nrow(trainData)-1)),names(cooksd),""), col="red")  # add labels
# Observations with Cook distance greater than 4/(n - p -1) where n is the number of observations and p is the number of input features should be eliminated to take care of the outliers
navalDataRefined3 <- trainData[abs(cooksd)<4/(nrow(trainData)-1),]
lmFitY2_Attempt3 <- lm(formula(lmFitY2_Attempt2), data = navalDataRefined3[,-14])
summary(lmFitY2_Attempt3)
mse_lmY1 <- mean(test[["Y1"]] - predict(lmFitY1_Attempt3, test))^2
mse_lmY1
# For univariate and bivariate model, outlier can be detected and treated using boxplots. For multi-variate model we use Cook's distance
cooksd <- cooks.distance(lmFitY2_Attempt2)
plot(cooksd, pch="*", cex=2, main="Influential Observations")  # plot cook's distance
abline(h = abs(4/(nrow(trainData)-1)), col="red")  # add cutoff line
text(x=1:length(cooksd)+1, y=cooksd, labels=ifelse(cooksd>abs(4/(nrow(trainData)-1)),names(cooksd),""), col="red")  # add labels
# Observations with Cook distance greater than 4/(n - p -1) where n is the number of observations and p is the number of input features should be eliminated to take care of the outliers
navalDataRefined3 <- trainData[abs(cooksd)<4/(nrow(trainData)-1),]
lmFitY2_Attempt3 <- lm(formula(lmFitY2_Attempt2), data = navalDataRefined3[,-14])
summary(lmFitY2_Attempt3)
lmFitY2_Attempt2 <- update(lmFitY2_Attempt1, ~ . - X2, data = train[,-14])
summary(lmFitY2_Attempt2)
mse_lmY1 <- mean(testData[["Y1"]] - predict(lmFitY1_Attempt3, testData))^2
mse_lmY1
mse_lmY2 <- mean(testData[["Y2"]] - predict(lmFitY2_Attempt3, testData))^2
mse_lmY2
navalDataPred <- read.csv("assign1_NavalPred.csv", header = TRUE)
navalDataPredFiltered = subset(navalDataPred,select = -c(X7,X9,X12))
# Y1 fitting has removal of X4, X7, X9, X12
# Y2 fitting has removal of X2, X7, X9, X12
navalDataPredY1 = subset(navalDataPredFiltered, select = -c(X4))
navalDataPredY2 = subset(navalDataPredFiltered, select = -c(X2))
# Predict using lm
lmY1Pred = predict(lmFinalFitY1,navalDataPredY1)
# Y1 fitting has removal of X4, X7, X9, X12
# Y2 fitting has removal of X2, X7, X9, X12
navalDataPredY1 = subset(navalDataPredFiltered, select = -c(X4))
navalDataPredY2 = subset(navalDataPredFiltered, select = -c(X2))
# Predict using lm
lmY1Pred = predict(lmFitY1_Attempt3,navalDataPredY1)
lmY2Pred = predict(lmFitY2_Attempt3,navalDataPredY2)
# Value of coefficient cannot be > 1
for(x in seq(1,1000)){
if(lmY1Pred[x]>1){
lmY1Pred[x] = 1
}
if(lmY2Pred[x]>1){
lmY2Pred[x] = 1
}
}
# Store and save predictions in .csv
lmYPredDataFrame <- data.frame("Y1"=lmY1Pred, "Y2"=lmY2Pred)
str(lmYPredDataFrame)
write.csv(lmYPredDataFrame, "assign1_NavalPred_LM.csv", row.names = TRUE)
navalDataRefined1 <- navalData[-c(9,12)]
cor(navalDataRefined1)
library(GGally)
ggcorr(navalDataRefined1, geom="tile")
# Plotting two-dimensional scatterplots of all pairs of variables
pairs(navalDataRefined1, pch = 19, col = "orange")
library(randomForest)
library(miscTools)
randomForestY1 <- randomForest(Y1 ~ ., data = trainData[,-15], importance = TRUE)
randomForestY2 <- randomForest(Y2 ~ ., data = trainData[,-14], importance = TRUE)
summary(randomForestY1)
summary(randomForestY2)
#Calculating the R squared and Mean Square Error using Random Forest Regressor.
r2_rf_Y1 <- rSquared(testData[["Y1"]], testData[["Y1"]] - predict (randomForestY1, testData))
r2_rf_Y2 <- rSquared(test[["Y2"]], testData[["Y2"]] - predict (randomForestY2, testData))
mse_rf_Y1 <- mean(testData[["Y1"]] - predict(randomForestY1, testData))^2
mse_rf_Y2 <- mean(testData[["Y2"]] - predict(randomForestY2, testData))^2
r2_rf_Y1
r2_rf_Y2
mse_rf_Y1
mse_rf_Y2
library(randomForest)
library(miscTools)
randomForestY1 <- randomForest(Y1 ~ ., data = trainData[,-15], importance = TRUE)
randomForestY2 <- randomForest(Y2 ~ ., data = trainData[,-14], importance = TRUE)
#summary(randomForestY1)
#summary(randomForestY2)
#Calculating the R squared and Mean Square Error using Random Forest Regressor.
r2_rf_Y1 <- rSquared(testData[["Y1"]], testData[["Y1"]] - predict (randomForestY1, testData))
r2_rf_Y2 <- rSquared(test[["Y2"]], testData[["Y2"]] - predict (randomForestY2, testData))
mse_rf_Y1 <- mean(testData[["Y1"]] - predict(randomForestY1, testData))^2
mse_rf_Y2 <- mean(testData[["Y2"]] - predict(randomForestY2, testData))^2
r2_rf_Y1
r2_rf_Y2
mse_rf_Y1
mse_rf_Y2
library(FNN)
knnY1 <- knn.reg(train = trainData[,-14:-15], test = testData[,-14:-15], y = trainData[["Y1"]])
knnY2 <- knn.reg(train = trainData[,-14:-15], test = testData[,-14:-15], y = trainData[["Y2"]])
#Calculating the R-squared and MSE value from the knn model.
mse_knn_Y1 <- mean(testData[["Y1"]] - knnY1$pred)^2
mse_knn_Y2 <- mean(testData[["Y2"]] - knnY2$pred)^2
r2_knn_Y1 <- rSquared(testData[["Y1"]], testData[["Y1"]] - knnY1$pred)
r2_knn_Y2 <- rSquared(testData[["Y2"]], testData[["Y2"]] - knnY2$pred)
mse_knn_Y1
mse_knn_Y2
r2_knn_Y1
r2_knn_Y2
library(FNN)
knnY1 <- knn.reg(train = trainData[,-14:-15], test = testData[,-14:-15], y = trainData[["Y1"]])
knnY2 <- knn.reg(train = trainData[,-14:-15], test = testData[,-14:-15], y = trainData[["Y2"]])
#Calculating the R-squared and MSE value from the knn model.
mse_knn_Y1 <- mean(testData[["Y1"]] - knnY1$pred)^2
mse_knn_Y2 <- mean(testData[["Y2"]] - knnY2$pred)^2
r2_knn_Y1 <- rSquared(testData[["Y1"]], testData[["Y1"]] - knnY1$pred)
r2_knn_Y2 <- rSquared(testData[["Y2"]], testData[["Y2"]] - knnY2$pred)
mse_knn_Y1
mse_knn_Y2
r2_knn_Y1
r2_knn_Y2
